---
layout: post
title: "PyTorch Lecture about Machine Learning - 3: Gradient Descent"
date: 2020-12-28
excerpt: "Sung Kim 님의 PyTorch lecture를 공부하고 작성하고자 합니다."
torch: true
tag:
- PyTorch
comments: false
---

- 지난 강의

1. [PyTorch Lecture about Machine Learning - 1: Overview]({% post_url 2020-12-27-lecture1 %})
2. [PyTorch Lecture about Machine Learning - 2: Linear Model]({% post_url 2020-12-27-lecture2 %})

## What is the learning

머신러닝에서의 learning은 **loss 값을 최소화하는 w(파라미터)를 찾는 것**이다.

이를 우리는 수학적으로 argminloss(w) 라 한다.<br>
즉, 파라미터를 찾아가는 과정 자체가 머신러닝이다.

## Gradient descent algorithm

우리는 올바른 w 값을 알지 못하기에 우리는 random point에서 시작한다.

random point에서 move in/ move out 할지 정하기 위해, w의 지점에서의 Gradient를 계산한다.<br>

![image](https://user-images.githubusercontent.com/28617444/103216857-6d072080-495a-11eb-979d-b167be6b4fd8.png)

이 Gradient가 양수이면, move in 한다. 그러면 w는 negative가 된다.
Gradient가 음수이면, w는 양수가 될 것이다.

이러한 과정이 noation으로 다음과 같다.

<center><img src="https://user-images.githubusercontent.com/28617444/103216899-84dea480-495a-11eb-954e-def43560d712.png" width="300" height="85"></center>

<br>

- 얼마나 move in 해야할까?

notation에 보이는 알파만큼 정의한다. 흔히 알고 있는 learning rate이다. (매우 작은 수)

이처럼 w를 업데이트해가며 Global loss minimum을 찾는다.

- 어떻게 Gradient를 계산할까?

수학적으로 그래프의 미분값을 기울기로 적용시키면 Gradient를 계산할 수 있다.

구체적으로 코드에서 구현해보겠다.

## Code 구현

```python
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = 1.0 #random value로 부여

#forward pass
def forward(x):
    return x * w

#loss function 계산식
def loss(x,y):
    y_pred = forward(x)
    return (y_pred - y) * (y_pred - y)

def gradient(x,y):
    return 2 * x * (x * w - y)


    # Before training
    print("Prediction (before training)",  4, forward(4))

    # Training loop
    for epoch in range(10):
        for x_val, y_val in zip(x_data, y_data):
            # Compute derivative w.r.t to the learned weights
            # Update the weights
            # Compute the loss and print progress
            grad = gradient(x_val, y_val)
            w = w - 0.01 * grad
            print("\tgrad: ", x_val, y_val, round(grad, 2))
            l = loss(x_val, y_val)
        print("progress:", epoch, "w=", round(w, 2), "loss=", round(l, 2))

    # After training
    print("Predicted score (after training)",  "4 hours of studying: ", forward(4))


```
![image](https://user-images.githubusercontent.com/28617444/103217781-cc663000-495c-11eb-83c1-284e97307c66.png)

gradient(x,y)를 보면 알 수 있듯이, 미분값을 구하기 위한 값을 반환한다.
또한, 이 때의 gradient값을 저장하여 learning rate (0.01 부여) 만큼 w를 업데이트하는 것을 확인할 수 있다.

epoch 는 학습 반복횟수로 현재 10번을 지정해주었다.

progress를 진행함에 따라, loss value는 점점 더 작아지며,
w는 2에 가까워짐을 확인할 수 있다.

매우 간단한 시스템이지만 powerful하며 자동적으로 w를 업데이트하는 알고리즘은 다양한 모델에 사용된다.
