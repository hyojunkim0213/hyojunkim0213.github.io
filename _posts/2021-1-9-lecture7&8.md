---
layout: post
title: "PyTorch Lecture about Machine Learning - 7&8: Wide & Deep model + DataLoader"
date: 2021-01-02
excerpt: "Sung Kim 님의 PyTorch lecture를 공부하고 작성하고자 합니다."
torch: true
tag:
- PyTorch
comments: false
---

- 지난 강의

1. [PyTorch Lecture about Machine Learning - 1: Overview]({% post_url 2020-12-27-lecture1 %})
2. [PyTorch Lecture about Machine Learning - 2: Linear Model]({% post_url 2020-12-27-lecture2 %})
3. [PyTorch Lecture about Machine Learning - 3: Gradient Descent]({% post_url 2020-12-28-lecture3 %})
4. [PyTorch Lecture about Machine Learning - 4: Back-propagation and Autograd]({% post_url 2020-12-30-lecture4 %})
5. [PyTorch Lecture about Machine Learning - 5 & 6: Linear Regression in PyTorch way]({% post_url 2020-12-30-lecture4 %})

## Matrix Multiplication

이전 시간까지는 input data가 1개였지만, 지금은 input data가 2차원인 경우의 계산을 살펴보겠다.<br>

이 때는 모델을 바꿀 필요는 없으며, 그저 **Matrix Multiplication**을 통해 해결할 수 있다.

아래 그림과 같이, X 값은 n by 2 의 matrix 형태로 입력되며, W를 곱하여 n by 1 의 matrix 인 y값이 도출된다.  
![image](https://user-images.githubusercontent.com/28617444/104092054-c78e6e00-52c4-11eb-92fa-b49c3c9be3f2.png)

N * 2 Matrix 에서 N * 1 Matrix 가 되게 하는 w는 2 * 1 matrix이다.
2차원 input 뿐만 아니라 여러 차원의 input에서도 똑같이 적용된다. <br>
즉, XW = Y 의 식이 성립된다.

```python
linear = torch.nn.Linear(2,1)
y_prd = linear(x_data)
```
코드에서의 (2,1) 은 input의 2차원과 output의 1차원을 의미한다.

이러한 한 층의 layer는 여러 layer를 쌓아 만들 수 있다.<br>
앞서 배웠던 Matrix Multiplication 의 방식으로 (2,4) * (4,3) * (3,1)이 계산되어 1차원의 출력층을 반환한다.<br>
즉, **서로 이어져있는 linear layer의 input size와 output size는 일치한다.**

또한, 모든 component를 연결하기 위해서 sigmoid 함수를 통해 값을 출력한다.
```python
sigmoid = torch.nn.Sigmoid()

l1 = torch.nn.Linear(2,4)
l2 = torch.nn.Linear(4,3)
l3 = torch.nn.Linear(3,1)

out1 = sigmoid(l1(x_data))
out2 = sigmoid(l2(out1))
y_pred = Sigmoid(13(out2))
```

## Sigmoid:Vanishing Gradient Problem

Sigmoid 함수를 통해서 학습을 진행하였지만, 그림처럼 다양한 층을 쌓으면 기울기 손실 문제(Vanishing Gradient Problem)이 발생한다.

![image](https://user-images.githubusercontent.com/28617444/104095887-9fab0480-52dc-11eb-93eb-89ae4466f996.png)

Sigmoid 함수는 0에서 1 사이의 작은 값을 반환하게 되고, 이러한 값들이 계속해서 **Multiplication** 을 거치면 매우 작아져 0에 근사하게 되어, backpropagation 시 Gradient를 소실하게 된다.

따라서 다양한 활성화 함수가 사용되며 우리가 흔히 사용하는 흔한 함수는 relu 함수이다.

## DataLoader

```python
xy = np.loadtxt('data-diabetes.csv', delimiter=',', dtype = np.float32)
x_data = Variable(torch.from_numpy(xy[:, 0:-1]))
y_data = Variable(torch.from_numpy(xy[:, [-1]))

for epoch in range(100):
  y_pred = model(x_data)

  loss = criterion(y_pred, y_data)
  print(epoch, loss.data[0])

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```

데이터셋의 양이 그렇게 많지않다면 위의 방식처럼 모든 데이터를 그대로 훈련시키면 된다.<br>
그러나, 수많은 데이터셋을 모두 모델에 훈련시킬 수 없다면 효율적인 방법은 데이터셋을 작은 Batch로 나누는 것이다.

```python
for epoch in range(training_epochs):
  for i in range(total_batch):
```

- one epoch : 모든 training data를 forward와 backward pass 한 상태
- batch size : forward/backward pass를 몇 개의 batch로 나누어 실행할 것인지
- iteration : batch size로 몇번 forward/backward 를 진행하는지의 횟수

1000개의 training example을 학습시킬때 batch size가 500이라면, 1 epoch를 실행하는데 2iteration이 걸릴 것이다.

이처럼 모든 데이터셋을 작은 batch 로 나누어 관리할 수 있지만, PyTorch의 **DataLoader** 를 통해 전체 process를 걱정할 필요 없이 그저 DataLoader를 통해 batch size만큼 데이터를 가져오면 된다.

```python
for i, data in enumerate(train_loader, 0):
  inputs, labels = data
  inputs, labels = Variable(inputs), Variable(labels)

class DiabetesDataset(Dataset):

  def __init__(self): #데이터를 다운로드하고 읽는 프로세스
        xy = np.loadtxt('data-diabetes.csv', delimiter = ',' , dtype = np.float32)
        self.len = xy.shape[0]
        self.x_data = torch.from_numpy(xy[:, 0:-1])
        self.y_data = torch.from_numpy(xy[:, [-1])

  def __getitem__(self, index): # index로 해당 아이템을 전달
    return self.x_data[index], self.y_data[index]

  def __len__(self): # 데이터 사이즈를 전달
    return self.len

##클래스 호출
dataset = DiabetesDataset()
train_loader = DataLoader(dataset = dataset,
                          batch_size =32
                          shuffle = True,
                          num_workers = 2)
```
