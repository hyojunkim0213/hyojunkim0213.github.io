---
layout: post
title: "PyTorch Lecture about Machine Learning - 5 & 6: Linear Regression in PyTorch way"
date: 2021-01-02
excerpt: "Sung Kim 님의 PyTorch lecture를 공부하고 작성하고자 합니다."
torch: true
tag:
- PyTorch
comments: false
---

- 지난 강의

1. [PyTorch Lecture about Machine Learning - 1: Overview]({% post_url 2020-12-27-lecture1 %})
2. [PyTorch Lecture about Machine Learning - 2: Linear Model]({% post_url 2020-12-27-lecture2 %})
3. [PyTorch Lecture about Machine Learning - 3: Gradient Descent]({% post_url 2020-12-28-lecture3 %})
4. [PyTorch Lecture about Machine Learning - 4: Back-propagation and Autograd]({% post_url 2021-1-2-lecture5&6 %})

## PyTorch Rhythm (lecture5)

이번 강의는 PyTorch를 더 유용하게 배워보고자 한다. PyTorch는 기본적으로 다음의 3가지 step에 따라 설계한다.

- step 1. Variable를 포함한 class를 이용하여 모델을 설계한다.
- step 2. loss와 optimizer를 구성한다.
- step 3. Training cycle (forward, backward, update를 하며 train한다.)

#### step 1 Model class in PyTorch way

먼저, data를 정의한다. (n by 1 )
```python
import torch
from torch.autograd import Variable

x_data = torch.tensor([ [1.0],[2.0],[3.0] ])
y_data = torch.tensor([ [2.0],[4.0],[6.0] ])

```

다음으로 Model class를 정의한다.
```python
class Model(torch.nn.Module): # subclass는 torch.nn.Module을 상속받는다.
  def __init__(self):
    # 먼저 초기화를 해준다. 이를 위해서, 부모 class로부터 상속받은 class를 부모의 init을 한다.
    super(Model, self).__init__()
    self.linear = torch.nn.Linear(1,1)
    #PyTorch API의 linear 모델을 구성
    #inputsize , outputsize = (1,1)

  def forward(self, x): #x를 입력받으면, y value를 예측하여 반환한다.
    y_pred = self.linear(x)
    return y_pred

#모델 호출
model = Model()
```

#### step 2 Construct loss and optimizer

linear regression 이므로, MSELoss를 호출한다. (편차 제곱의 평균)<br>
또한, 지난 시간에는 gradient의 계산과정을 통해 w를 업데이트 해주었는데 이번에는 optimizer로 SGD알고리즘(stochastic gradient descent)을 사용한다. 이를 통해 파라미터를 쉽게 업데이트할 수 있다.
```python
criterion= torch.nn.MSELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)
```

#### step 3 Training: forward, loss, backward, step

epoch cycle만큼 training loop를 돌며 train하는 과정이다.
```python
for epoch in range(500):
  y_pred = model(x_data) # 모든 것을 지나가며, y_pred를 저장한다.

  loss = criterion(y_pred, y_data) # step2 에서 정의한 criterion function을 불러와 predict데이터와 실제 데이터의 loss를 계산한다.
  print(epoch, loss.data[0])

  #위에서의 과정은 forward pass 과정이며 이제 backward pass를 진행한다.
  optimizer.zero_grad() # 모든 gradient에 대해 initialize해준다.
  loss.backward() #구한 loss로부터 back propagation을 진행하여 loss에 대한 gradient을 계산한다.
  optimizer.step() #미리 선언한 model의 파라미터들이 업데이트 된다.
```

#### step 4 Testing Model

3가지 step이 진행된 후, 모델을 평가하기 위해 model.forward를 호출한다.

```python
hour_var = Variable(torch.Tensor([[4.0]]))
print("predict",4,model.forward(hour_var).data[0][0])
```

이처럼 기본 아키텍처는 위에서 정의한 step과 동일하다.

이러한 아키텍처를 토대로, 모델을 구성하고 train시켜 활용한다.

특히, 쥬피터 노트북 환경에 익숙했던 나에게 이러한 **모델 구조를 하나하나 파악하는 것**은 중요한 일이라고 생각한다.

실무에서는 쥬피터 노트북 환경보다 리눅스 또는 cmd 환경에서 .py 파일을 불러와서 바로 실행시켜가며 개선하고자 할때, 이러한 구조를 모르면 전혀 응용하지 못할 것 이다.

따라서, 이 강의를 토대로 앞으로 다양한 모델의 구조를 뜯어보며 리뷰할 계획이다.

## Logistic Regression (lecture6)

실제 현실에서는 일반적인 값 예측보다 Binary prediction (0 / 1)의 경우가 매우 유용하다.

pass 할지 / fail 할지, win / lose , propose / not 등 다양한 Binary 문제가 존재한다.

이를 해결하기 위해, linear 모델의 마지막층에 sigmoid 함수를 추가해주는 것이다.

- Sigmoid 함수

다음 그래프는 sigmoid 함수를 나타내며, sigmoid 값은 0과 1 사이의 값을 가지는 것을 확인할 수 있다. 즉, output값이 x축으로 들어오면, y축의 값으로 반환한다고 생각하면 된다. <br>
지금껏 해온 binary classification project를 보면 output층이 sigmoid 함수로 구성되어 있는 것을 확인할 수 있다. (간단한 아이디어가 임계값을 정하는 것이며, 대부분 임계값이 0.5 이상이면 1을 반환, 0.5 이하이면 0을 반환하는 것이다. )

![image](https://user-images.githubusercontent.com/28617444/103454982-7c0c1b00-4d2c-11eb-99fd-556aae1a34a0.png)

그렇다면, binary classification에서 loss값은 어떻게 계산할 수 있을까?

- Cross entroypy

![image](https://user-images.githubusercontent.com/28617444/103456514-761d3680-4d3a-11eb-976f-344720edb354.png)

수식을 보면 알 수 있듯이, loss value가 작을수록 예측을 잘 한 것이며, loss value가 클수록 예측을 잘 못한 것이라 판단할 수 있다.

## Code

Sigmoid 함수는 **torch.nn.functional.sigmoid** 를 통해 쉽게 호출할 수 있다.

데이터를 정의할 때는 logistic regression이므로 y_data는 0 또는 1의 값을 가진다.

```python
import torch.nn.functional as F

x_data = Variable(torch.Tensor([1.0],[2.0],[3.0],[4.0]))
y_data = Variable(torch.Tensor([[0.],[0.],[1.],[1.]]))

class Model(torch.nn.Module):

  def __init__(self):
    super(Model, self).__init__()
    self.linear = torch.nn.Linear(1,1) #Linear one input, one output

  def forward(self, x):
    y_pred = F.sigmoid(self.linear(x)) # linear output에 sigmoid 함수 취한다.
    return y_pred

```
또한, loss 계산할 시에는 **torch.nn.BCELoss**를 호출하여 Binary Cross Entroypy loss를 사용한다.

```python
model = Model()

criterion = torch.nn.BCELoss(size_average =True)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

#트레이닝 루프
for epoch in range(1000):
  y_pred = model(x_data)

  loss = criterion(y_pred, y_data)
  print(epoch, loss.data[0])

 optimizer.zero_grad()
 loss.backward()
 optimizer.step()

#트레이닝 후 Test

hour_var = Variable(torch.Tensor([[1.0]]))
print("predict 1 hour", 1.0, model(hour_var).data[0][0] > 0.5)
hour_var = Variable(torch.Tensor([[7.0]])
print("predict 7 hour", 7.0, model(hour_var).data[0][0] > 0.5)

```
